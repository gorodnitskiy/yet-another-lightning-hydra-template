# https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.EarlyStopping.html

# Monitor a metric and stop training when it stops improving.
# Look at the above link for more detailed information.

# To avoiding copying of loss and metric names, during hydra initialization
# there is custom resolver which replaces __loss__ to loss.__class__.__name__
# and __metric__ to main_metric.__class__.__name__,
# for example: ${replace:"__metric__/valid"}
# Use quotes for defining internal value in ${replace:"..."} to avoid
# grammar problems with hydra config parser.

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: ${replace:"__metric__/valid"} # quantity to be monitored, must be specified !!!
  min_delta: 5.0e-5 # minimum change in the monitored quantity to qualify as an improvement
  patience: 15 # number of checks with no improvement after which training will be stopped
  verbose: False # verbosity mode
  mode: "max" # "max" means higher metric value is better, can be also "min"
  strict: True # whether to crash the training if monitor is not found in the validation metrics
  check_finite: True # when set True, stops training when the monitor becomes NaN or infinite
  stopping_threshold: null # stop training immediately once the monitored quantity reaches this threshold
  divergence_threshold: null # stop training as soon as the monitored quantity becomes worse than this threshold
  check_on_train_epoch_end: null # whether to run early stopping at the end of the training epoch
  # log_rank_zero_only: False  # this keyword argument isn't available in stable version
